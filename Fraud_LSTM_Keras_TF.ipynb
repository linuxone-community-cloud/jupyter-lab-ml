{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection in Credit Card transactions with LSTM model\n",
    "In this demo, we will show Tensorflow functionality on IBM Z/LinuxONE using credit card transactional data in CSV format provided by IBM in kaggle.com. The data contains normal and also transactions that are likely fraudulent with patterns in historical data such as time since last transaction, amount of money transferred, location (city, country), type of card payment (online/offline), etc. The data is transformed and joined in a Pandas DataFrame, then split into training, validation and test datasets. A Long Short-Term Memory (LSTM) algorithm is then used to predict Fraud Yes/No in the transactions.\n",
    "\n",
    "Gates are part of the hidden mechanics of the LSTM layer in Tensorflow/Keras. LSTM model in this demo will be built with 2 hidden LSTM layers. Each layer implicitly consists of the gates as depicted below.  \n",
    "\n",
    "![LSTM CC Diagram](<./LSTM for CC on IBM Telum Chip.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we will set up a machine learning environment by importing key libraries like TensorFlow (with Keras), NumPy, Pandas, and others for data processing, math operations, model saving/loading, and visualization. We also configures TensorFlow to suppress non-critical log messages and output the TensorFlow version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import joblib\n",
    "import pydot\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 7 # Six past transactions followed by current transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset into Pandas DataFrame\n",
    "In this section, we'll demonstrate how to load credit card transactions from a CSV file using the Pandas library, convert columns as needed, remove duplicates, and split the dataset into training (50%), validation (30%), and testing sets (20%). Then we generate the indices for each set using a random selection approach. The resulting indices can be used to access specific rows from the original dataframe or perform other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv('transactions_history.csv')\n",
    "tdf['Merchant Name'] = tdf['Merchant Name'].astype(str)\n",
    "tdf.sort_values(by=['User','Card'], inplace=True)\n",
    "tdf.reset_index(inplace=True, drop=True)\n",
    "print (tdf.info())\n",
    "\n",
    "# Get first of each User-Card combination\n",
    "first = tdf[['User','Card']].drop_duplicates()\n",
    "f = np.array(first.index)\n",
    "\n",
    "# Drop the first N transactions\n",
    "drop_list = np.concatenate([np.arange(x,x + timesteps - 1) for x in f])\n",
    "index_list = np.setdiff1d(tdf.index.values,drop_list)\n",
    "\n",
    "# Split into 0.5 train, 0.3 validate, 0.2 test\n",
    "tot_length = index_list.shape[0]\n",
    "train_length = tot_length // 2\n",
    "validate_length = (tot_length - train_length) * 3 // 5\n",
    "test_length = tot_length - train_length - validate_length\n",
    "print (tot_length,train_length,validate_length, test_length)\n",
    "\n",
    "# Generate list of indices for train, validate, test\n",
    "np.random.seed(1111)\n",
    "train_indices = np.random.choice(index_list, train_length, replace=False)\n",
    "tv_list = np.setdiff1d(index_list, train_indices)\n",
    "validate_indices = np.random.choice(tv_list, validate_length, replace=False)\n",
    "test_indices = np.setdiff1d(tv_list, validate_indices)\n",
    "print (train_indices, validate_indices, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a test sample as original dataset + test indices\n",
    "Indices are necessary to speed up large data processing. They play a crucial role in efficient data handling, especially when working with large datasets like millions of transaction records. Indexing enables quick access to rows and efficient joins/merges, optimizes memory usage and eliminates repeated sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sample(df, indices):\n",
    "    print(indices)\n",
    "    rows = indices.shape[0]\n",
    "    index_array = np.zeros((rows, timesteps), dtype=int)\n",
    "    for i in range(timesteps):\n",
    "        index_array[:,i] = indices + 1 - timesteps + i\n",
    "    uniques = np.unique(index_array.flatten())\n",
    "    df.loc[uniques].to_csv('test_220_40k.csv',index_label='Index')\n",
    "    np.savetxt('test_220_40k.indices',indices.astype(int),fmt='%d')\n",
    "\n",
    "create_test_sample(tdf, validate_indices[:40000]) # Uncomment this line to generate a test sample                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom mapping functions\n",
    "To customize how text columns are treated, we define custom mapping (encoder) functions using the astype and to_datetime methods in Pandas. These mapping functions can transform specific categories or values within a column to numerical representations of a specific type, allowing for more nuanced analysis and modeling. For example, a function amtEncoder that maps credit card transaction value in USD with $ to a float type value suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeEncoder(X):\n",
    "    X_hm = X['Time'].str.split(':', expand=True)\n",
    "    d = pd.to_datetime(dict(year=X['Year'],month=X['Month'],day=X['Day'],hour=X_hm[0],minute=X_hm[1])).astype(int)\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "def amtEncoder(X):\n",
    "    amt = X.apply(lambda x: x[1:]).astype(float).map(lambda amt: max(1,amt)).map(math.log)\n",
    "    return pd.DataFrame(amt)\n",
    "\n",
    "def decimalEncoder(X,length=5):\n",
    "    dnew = pd.DataFrame()\n",
    "    for i in range(length):\n",
    "        dnew[i] = np.mod(X,10) \n",
    "        X = np.floor_divide(X,10)\n",
    "    return dnew\n",
    "\n",
    "def fraudEncoder(X):\n",
    "    return np.where(X == 'Yes', 1, 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit pre-processing function using DataFrameMapper and pickle the mapper\n",
    "In machine learning, pre-processing transforms raw data into a structured format by handling missing values, scaling features, encoding categorical variables, and addressing imbalanced data. It also involves tasks like feature engineering, outlier removal, and dimensionality reduction to enhance model performance and accuracy. These steps ensure the data is clean and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'shared/fraud_lstm'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mapper = DataFrameMapper([('Is Fraud?', FunctionTransformer(fraudEncoder)),\n",
    "                          (['Merchant State'], [SimpleImputer(strategy='constant'), FunctionTransformer(np.ravel),\n",
    "                                               LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          (['Zip'], [SimpleImputer(strategy='constant'), FunctionTransformer(np.ravel),\n",
    "                                     FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('Merchant Name', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('Merchant City', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          ('MCC', [LabelEncoder(), FunctionTransformer(decimalEncoder), OneHotEncoder()]),\n",
    "                          (['Use Chip'], [SimpleImputer(strategy='constant'), LabelBinarizer()]),\n",
    "                          (['Errors?'], [SimpleImputer(strategy='constant'), LabelBinarizer()]),\n",
    "                          (['Year','Month','Day','Time'], [FunctionTransformer(timeEncoder), MinMaxScaler()]),\n",
    "                          ('Amount', [FunctionTransformer(amtEncoder), MinMaxScaler()])\n",
    "                         ], input_df=True, df_out=True)\n",
    "mapper.fit(tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize and deserialize fitted mapper as Python objects using pickle module. This allows you to save a preprocessing-function to a file and load it later without having to retrain it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mapper, open(os.path.join(save_dir,'fitted_mapper.pkl'),'wb'))\n",
    "mapper = joblib.load(open(os.path.join(save_dir,'fitted_mapper.pkl'),'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a transform on one row to get number of mapped features (including label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapped_sample = mapper.transform(tdf[:100])\n",
    "mapped_size = mapped_sample.shape[-1]\n",
    "print(mapped_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters():\n",
    "    total = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        parameters = 1\n",
    "        for dim in shape:\n",
    "            parameters *= dim\n",
    "        total += parameters\n",
    "        print (variable, shape, parameters)\n",
    "    print(total)\n",
    "\n",
    "def f1(conf):\n",
    "    precision = float(conf[1][1]) / (conf[1][1]+conf[0][1])\n",
    "    recall = float(conf[1][1]) / (conf[1][1]+conf[1][0])\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "class TP(tf.keras.metrics.TruePositives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class FP(tf.keras.metrics.FalsePositives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class FN(tf.keras.metrics.FalseNegatives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n",
    "\n",
    "class TN(tf.keras.metrics.TrueNegatives):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        super().update_state(y_true[-1,:,:], y_pred[-1,:,:], sample_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and compiling the LSTM model with Tensorflow\n",
    "LSTM (Long Short-Term Memory) needs to be defined first with a set of paramaters using integrated Keras in Tensorflow for simplicity. \n",
    "```input_shape=tf_input``` - is a matrix of ```timesteps=7``` the number of time steps per input sequence, and ```input_size``` is the number of features per time step.\n",
    "Training and Testing is completed in batches of 16. \n",
    "\n",
    "Using visualization function within the Keras library - ```plot_model``` will create a graphical representation of the LSTM model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [200,200]\n",
    "input_size = mapped_size - 1\n",
    "output_size = 1\n",
    "batch_size = 16\n",
    "timesteps = 7\n",
    "tf_input = ([timesteps, input_size])\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units[0], input_shape=tf_input, batch_size=batch_size, time_major=True, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units[1], return_sequences=True, time_major=True),\n",
    "    tf.keras.layers.Dense(output_size, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.summary()\n",
    "tf.keras.utils.plot_model(lstm_model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "metrics=['accuracy', \n",
    "    TP(name='TP'),\n",
    "    FP(name='FP'),\n",
    "    FN(name='FN'),\n",
    "    TN(name='TN'),\n",
    "    tf.keras.metrics.TruePositives(name='tp'),\n",
    "    tf.keras.metrics.FalsePositives(name='fp'),\n",
    "    tf.keras.metrics.FalseNegatives(name='fn'),\n",
    "    tf.keras.metrics.TrueNegatives(name='tn')\n",
    "   ]\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator for training batches\n",
    "Generator function produces one batch of data at a time using indices, instead of loading everything into memory at once. This allows streaming of large datasets for on-the-fly processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_batch(df, mapper, index_list, batch_size):\n",
    "    np.random.seed(98765)\n",
    "    train_df = df.loc[index_list]\n",
    "    non_fraud_indices = train_df[train_df['Is Fraud?'] == 'No'].index.values\n",
    "    fraud_indices = train_df[train_df['Is Fraud?'] == 'Yes'].index.values\n",
    "    fsize = fraud_indices.shape[0]\n",
    "    while True:\n",
    "        indices = np.concatenate((fraud_indices,np.random.choice(non_fraud_indices,fsize,replace=False)))\n",
    "        np.random.shuffle(indices)\n",
    "        rows = indices.shape[0]\n",
    "        index_array = np.zeros((rows, timesteps), dtype=int)\n",
    "        for i in range(timesteps):\n",
    "            index_array[:,i] = indices + 1 - timesteps + i\n",
    "        full_df = mapper.transform(df.loc[index_array.flatten()])\n",
    "        target_buffer = full_df['Is Fraud?'].to_numpy().reshape(rows, timesteps, 1)\n",
    "        data_buffer = full_df.drop(['Is Fraud?'],axis=1).to_numpy().reshape(rows, timesteps, -1)\n",
    "\n",
    "        batch_ptr = 0\n",
    "        while (batch_ptr + batch_size) <= rows:\n",
    "            data = data_buffer[batch_ptr:batch_ptr+batch_size]\n",
    "            targets = target_buffer[batch_ptr:batch_ptr+batch_size]\n",
    "            batch_ptr += batch_size\n",
    "            yield data,targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow training - LSTM model\n",
    "Training the LSTM model via forward pass, loss calculation, backward pass, weights update, epoch loop and validation after each epoch. In ModelCheckpoint, after each epoch, Tensorflow checks the monitored metric  and if the monitored metric improves, the model is saved to the specified location. The best model is then retrieved from checkpoints and saved in the ```saved_dir```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 500\n",
    "checkpoint_dir = \"./checkpoints/ccf_220_keras_lstm_static/\"\n",
    "filepath = checkpoint_dir + \"iter-{epoch:02d}/model.ckpt\"\n",
    "batch_size = 16\n",
    "\n",
    "print (\"Learning...\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, save_weights_only=True, verbose=1)\n",
    "train_generate = gen_training_batch(tdf,mapper,train_indices,batch_size)\n",
    "lstm_model.fit(train_generate, epochs=5, steps_per_epoch=steps_per_epoch, verbose=1, callbacks=[cp_callback])\n",
    "\n",
    "lstm_model.save_weights(os.path.join(save_dir,\"wts\"))\n",
    "lstm_model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator for validation/test batches\n",
    "In similar to the generator used in training process, we will use the generator function for test batches during the testing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_batch(df, mapper, indices, batch_size):\n",
    "    rows = indices.shape[0]\n",
    "    index_array = np.zeros((rows, timesteps), dtype=int)\n",
    "    for i in range(timesteps):\n",
    "        index_array[:,i] = indices + 1 - timesteps + i\n",
    "    count = 0\n",
    "    while (count + batch_size <= rows):        \n",
    "        full_df = mapper.transform(df.loc[index_array[count:count+batch_size].flatten()])\n",
    "        data = full_df.drop(['Is Fraud?'],axis=1).to_numpy().reshape(batch_size, timesteps, -1)\n",
    "        targets = full_df['Is Fraud?'].to_numpy().reshape(batch_size, timesteps, 1)\n",
    "        count += batch_size\n",
    "        yield data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow testing - LSTM model\n",
    "During the model testing step in TensorFlow/Keras, we will evaluate the trained LSTM model on unseen data (a test set) to measure its final performance.The model is loaded from the ```saved_model``` location, compiled and then tested via ```evaluate``` function.\n",
    "We will use small and bigger batches for a Quick test and Full test accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2000\n",
    "\n",
    "input_size=mapped_size-1\n",
    "output_size=1\n",
    "units=[200,200]\n",
    "timesteps = 7\n",
    "tf_input = ([timesteps, input_size])\n",
    "\n",
    "new_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(units[0], input_shape=tf_input, batch_size=batch_size, time_major=True, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units[1], return_sequences=True, time_major=True),\n",
    "    tf.keras.layers.Dense(output_size, activation='sigmoid')\n",
    "])\n",
    "new_model.load_weights(os.path.join(save_dir,\"wts\"))\n",
    "new_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "ddf = pd.read_csv('test_220_40k.csv', dtype={\"Merchant Name\":\"str\"}, index_col='Index')\n",
    "indices = np.loadtxt('test_220_40k.indices')\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "print(\"\\nQuick test\")\n",
    "test_generate = gen_test_batch(ddf,mapper,indices,batch_size)\n",
    "new_model.evaluate(test_generate, verbose=1)\n",
    "\n",
    "print(\"\\nFull test\")\n",
    "test_generate = gen_test_batch(tdf,mapper,test_indices,batch_size)\n",
    "new_model.evaluate(test_generate, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX\n",
    "tf2onnx is a tool that converts TensorFlow models into ONNX (Open Neural Network Exchange) format, making them compatible with other platforms and backends. ONNX is an open-source format designed for interoperability across different deep learning frameworks, allowing models trained in TensorFlow (or other frameworks like PyTorch) to be used on different platforms without needing to be re-trained or re-implemented. In our context here, models trained on x86 can be exported to ONNX and deployed on IBM Z/LinuxONE for inferencing. \n",
    "\n",
    "![LSTM CC Diagram](<./Train Anywhere Inference on IBM Z.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into ONNX format\n",
    "from tensorflow import keras\n",
    "import tf2onnx\n",
    "import onnx\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# convert to onnx\n",
    "onnx_model,_ = tf2onnx.convert.from_keras(new_model, opset=13)\n",
    "\n",
    "# Save model in ONNX format\n",
    "onnx.save_model(onnx_model, \"fraud_lstm.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
