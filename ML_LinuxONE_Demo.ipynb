{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Retention Demo Using Python\n",
    "In this demo, we will show Anaconda functionality on IBM LinuxONE accessing data from CSV files. The data in customer CSV consists of 6,001 rows of customer information.  The data in transaction CSV consists of 20,000 rows of transaction data. The data is transformed and joined in a Pandas DataFrame, which is used to perform exploratory analyses. A random forest algorithm is then used to predict customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from CSVs to Pandas DataFrames\n",
    "This step will load the CSV files into Pandas DataFrames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Credit card transactions***\n",
    "\n",
    "Load credit card transactions into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "txn_df = pd.read_csv('transactions.csv')\n",
    "print(txn_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "txn_df['TX_TOTAL_AMT'] = pd.to_numeric(txn_df['TX_TOTAL_AMT'])\n",
    "txn_df['CONT_ID'] = txn_df['CONT_ID'].astype('int64')\n",
    "txn_df['DATE_CREDIT'] = pd.to_datetime(txn_df['DATE_CREDIT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "txn_df['date'] = txn_df['DATE_CREDIT'].apply(lambda x: x.date())\n",
    "txn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Client Data***\n",
    "\n",
    "Load client data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "client_df = pd.read_csv('clients.csv')\n",
    "client_df = client_df.set_index(\"CONT_ID\")\n",
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate statistics\n",
    "Calculate a few aggregate statistics based on credit transactions and join the results to the client data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Total transactions per customer\n",
    "total_txns_df = txn_df.groupby('CONT_ID').size().rename(\"total_txns\").to_frame()\n",
    "client_df = total_txns_df.join(client_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Total transaction amounts per customer\n",
    "total_txn_amount_df = txn_df.groupby('CONT_ID')['TX_TOTAL_AMT'].sum().rename(\"total_txn_amount\").to_frame()\n",
    "client_df = client_df.join(total_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Average transaction amounts per customer\n",
    "avg_txn_amount_df = txn_df.groupby('CONT_ID')['TX_TOTAL_AMT'].mean().rename(\"avg_txn_amount\").to_frame()\n",
    "client_df = client_df.join(avg_txn_amount_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Average daily transactions per customer\n",
    "daily_txns = txn_df.groupby(['date', 'CONT_ID']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Missing transactions on a particular day means customer had none.\n",
    "# These days should be included in the average as 0 transaction days.\n",
    "avg_daily_txns_df = daily_txns.unstack().fillna(0).mean().rename(\"avg_daily_txns\").to_frame()\n",
    "client_df = client_df.join(avg_daily_txns_df)\n",
    "client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analyses\n",
    "We begin our exploration of the data set by creating a scatterplot of ANNUAL_INCOME vs. AGE_YEARS and their associated histograms. Matplotlib and Seaborn are two common plotting libraries used in Python.  These plotting libraries are useful in creating custom visualizations to help gain insights from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jointplot(x, y, data, **kwargs):\n",
    "    size = kwargs.pop('size', 10)\n",
    "    alpha = kwargs.pop('alpha', 0.3)\n",
    "    return sns.jointplot(x=x, y=y, data=data, \n",
    "                         alpha=alpha,\n",
    "                         size=size,\n",
    "                         **kwargs)\n",
    "\n",
    "# for widget\n",
    "def w_jointplot(x, y):\n",
    "    g = jointplot(x, y, filter_outliers(client_df, by_col=y))\n",
    "    plt.close()\n",
    "    return g.fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "churn_labels = ['Did not churn', 'Did churn']\n",
    "\n",
    "def filter_outliers(d, by_col=None):\n",
    "    if isinstance(d, pd.\n",
    "                  Series):\n",
    "        return d[((d-d.mean()).abs()<=3*d.std())]\n",
    "    elif isinstance(d, pd.DataFrame):\n",
    "        if not by_col:\n",
    "            raise ValueError('by_col is required for DataFrame')\n",
    "        return d[np.abs(d[by_col]-d[by_col].mean())<=(3*d[by_col].std())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ax = jointplot('AGE_YEARS', 'ANNUAL_INCOME', filter_outliers(client_df, by_col='ANNUAL_INCOME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "Next, we compute the correlation coefficients between each variable and create a color-coded correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corr = client_df.corr()\n",
    "\n",
    "# only show lower triangle\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,12))\n",
    "ax = sns.heatmap(corr, mask=mask, square=True, annot=True, fmt='.2f',\n",
    "                 cbar=True,\n",
    "                 ax=ax)\n",
    "title = ax.set_title('Correlations', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn\n",
    "Here we plot the distributions of clients who did and did not churn. The green histogram shows the number of clients who did churn. The blue histogram shows the number of clients who did not churn.  The line graphs show the density functions for each case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_churn_by(df, col, **kwargs):\n",
    "    f, ax = plt.subplots(figsize=(12,10), sharex=True)\n",
    "    kde = kwargs.get('kde', False)\n",
    "    hist = kwargs.get('hist', False)\n",
    "    for churn in df.CHURN.unique():\n",
    "        sns.distplot(df[df.CHURN == churn][col], \n",
    "                     label=churn_labels[churn], \n",
    "                     kde_kws={'shade': (kde and not hist)},\n",
    "                     ax=ax, \n",
    "                     **kwargs)\n",
    "\n",
    "    ax.set_title('Client Churn by {}'.format(col))\n",
    "    label = ax.set_xlabel('{}'.format(col))\n",
    "    return f, ax\n",
    "\n",
    "def w_plot_churn_by(column, hist=True, kde=False, norm_hist=False):\n",
    "    df = filter_outliers(client_df, by_col=column)\n",
    "    f, ax = plot_churn_by(df, column, hist=hist, kde=kde, norm_hist=norm_hist)\n",
    "    plt.legend()\n",
    "    plt.close()\n",
    "    return f\n",
    "\n",
    "f, ax = plot_churn_by(client_df, 'AGE_YEARS')\n",
    "ax = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the correlation matrix above, the two features that showed a negative correlation with churn were age and activity level. Here we generate a boxplot with those two features as the axes, and churn as the category. The plot shows that clients that churn tend to be younger across all levels of activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "col = 'AGE_YEARS'\n",
    "data = filter_outliers(client_df, by_col=col)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(x='ACTIVITY_LEVEL', y=col, hue=\"CHURN\", data=data, \n",
    "                 palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This beeswarm plot shows clients binned by the level of activity they maintain with the bank. Clients that churned maintained lower levels of activity (0-2). And of clients within these lower activity levels, younger clients churned more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "ax = sns.swarmplot(x='ACTIVITY_LEVEL', y='AGE_YEARS', hue='CHURN', \n",
    "                   data=data.sample(n=100, random_state=51), \n",
    "                   palette='muted', ax=ax)\n",
    "title = ax.set_title('Client Churn by Activity Level')\n",
    "label = ax.set_ylabel('Age (Years)')\n",
    "label = ax.set_xlabel('Activity Level')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles, churn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train churn model\n",
    "We now start to do some predictive analyses on the data to evaluate customer churn. To keep things simple, we use a single data set, which we split into training and test data sets. We use the training data to train the model, and the test data to make predictions about lost revenue to the bank.\n",
    "\n",
    "We use a supervised learning algorithm, random forest, to train the model. Random Forest is a popular algorithm for both classification and regression. It requires very little tuning and is less prone to overfitting. Random forest is an aggregation of decision trees where each tree classifies an observation in a dataset. Since random forest aggregates many classifiers, it is considered an ensemble method. Using scikit learn, we create our random forest for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_feature_space(df):\n",
    "    '''Create the feature space required by our classifier.'''\n",
    "    # drop columns/features we don't want/need for the classifier\n",
    "    features_df = df.drop(['CHURN', 'CUSTOMER_ID'], axis=1, errors='ignore')\n",
    "    X = features_df.to_numpy()\n",
    "    # normalize feature values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "def predict_churn(X):\n",
    "    '''Predict the probabilit of churn from feature set.'''\n",
    "    return clf.predict_proba(X)[:,1]\n",
    "\n",
    "def train_model(X, y):\n",
    "    '''Train our classifier using features X and target variable y.'''\n",
    "    clf = RF(n_estimators=100)\n",
    "    return clf.fit(X, y)\n",
    "\n",
    "# Train the model\n",
    "# split data into train, test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(client_df, test_size=0.2)\n",
    "\n",
    "# target variable\n",
    "y = np.array(train_df['CHURN'])\n",
    "\n",
    "# extract features\n",
    "X = make_feature_space(train_df)\n",
    "\n",
    "# train classifier\n",
    "clf = train_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, the churn classifier and the test data set are used for our churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export trained model into ONNX format \n",
    "sklearn-onnx converts models in ONNX format which can be then used to compute predictions with another backend on a different platform. sci-learn is supported on most of the platforms including IBM LinuxONE. So there is no need to do it. However for any platform specific models it's the way to expand the range of supported platforms.  A non-IBM LinuxONE can be exported to ONNX and then executed on IBM LinuxONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into ONNX format\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
    "onx = convert_sklearn(clf, initial_types=initial_type)\n",
    "with open(\"customer_churn.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate business loss\n",
    "In this simple example, we calculate the predicted loss of business (revenue) for all clients in the test data set. We calculate the revenue from each client, and multiply that by the churn probability to determine the predicted loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calc_business_loss_random_forest(df):\n",
    "    df['CUSTOMER_ID'] = df.index\n",
    "    data = df.copy()\n",
    "\n",
    "    # extract features\n",
    "    X = make_feature_space(df)\n",
    "    \n",
    "    # predict churn\n",
    "    data['churn_probability'] = predict_churn(X)\n",
    "    \n",
    "    # TODO: avg_daily_balance would be a nice feature to have here\n",
    "    # for now, we'll just use fraction of income\n",
    "    avg_daily_balance = df['ANNUAL_INCOME'] / 6\n",
    "\n",
    "    # Interest made on deposits\n",
    "    deposit_rate = 0.02\n",
    "\n",
    "    # Fee collected for each credit txn\n",
    "    credit_rate = 0.015\n",
    "\n",
    "    # Assume we make some money on trading fees and/or portfolio management\n",
    "    mgmt_rate = 0.02\n",
    "\n",
    "    # How much is each customer worth to the business?\n",
    "    worth = deposit_rate * avg_daily_balance + \\\n",
    "            mgmt_rate * df['ANNUAL_INVEST'] + \\\n",
    "            credit_rate * df['total_txn_amount']\n",
    "    data['worth'] = worth\n",
    "    \n",
    "    # How much would we lose per annum?\n",
    "    data['predicted_loss'] = data['churn_probability'] * worth\n",
    "    \n",
    "    return data.sort_values(by='predicted_loss', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "churn_df = calc_business_loss_random_forest(test_df)\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss by Age Group\n",
    "In this section, we calculate and plot the predicted loss of revenue by age group. In our data set, age is an important feature in predicting if a client will churn. We create a DataFrame containing the cumulative predicted loss by age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def group_by_age(df, bins=None):\n",
    "    if bins is None:\n",
    "        bin_size = 5\n",
    "        _min, _max = int(df.AGE_YEARS.min()), int(df.AGE_YEARS.max())\n",
    "        bins = range(_min, _max + bin_size, 5)\n",
    "    return df.groupby(pd.cut(df.AGE_YEARS, bins=bins))\n",
    "\n",
    "data_by_age = churn_df.pipe(group_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_by_age_df = data_by_age['predicted_loss'].sum().reset_index()\n",
    "loss_by_age_df['AGE_YEARS'] = loss_by_age_df['AGE_YEARS'].astype(str)\n",
    "\n",
    "loss_by_age_df.plot(x='AGE_YEARS', y='predicted_loss', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analyses\n",
    "\n",
    "We now start to do some predictive analyses on the data to evaluate cutomer churn based on activity level. We use a supervised learning algorithm, logistic regression, to train the model. Logistic regression is a common, fast, highly scalable, classification model that doesn't require much tuning and is easy to regularize. The model outputs a set of probabilities which can be more useful than class labels. Here, we will use PyTorch Machine Learning library to create our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch works with tensors. So, the first steps is to convert train and test datasets into tensors using the “torch.from_numpy()” method. The datatype should be converted into float32 before this. Which can be done using “astype()” function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "scaler = MinMaxScaler()\n",
    "#'ACTIVITY_LEVEL','total_txn_amount','avg_txn_amount','avg_daily_txns'\n",
    "x_train=torch.from_numpy(scaler.fit_transform(train_df[['ACTIVITY_LEVEL']].astype(np.float32).values))\n",
    "x_test=torch.from_numpy(scaler.fit_transform(test_df[['ACTIVITY_LEVEL']].astype(np.float32).values))\n",
    "y_train=torch.from_numpy(train_df['CHURN'].astype(np.float32).values)\n",
    "y_test=torch.from_numpy(test_df['CHURN'].astype(np.float32).values)\n",
    "#\n",
    "n_features = 1\n",
    "#\n",
    "y_train=y_train.view(y_train.shape[0],1)\n",
    "y_test=y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a custom model in PyTorch for logistic regression. For that we define a class with the model name. This class should derive torch.nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(torch.nn.Module):\n",
    " def __init__(self,no_input_features):\n",
    "   super(LogisticRegressionModel,self).__init__()\n",
    "   self.linear=torch.nn.Linear(no_input_features,1)\n",
    " def forward(self,x):\n",
    "   y_predicted=torch.sigmoid(self.linear(x))\n",
    "   return y_predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the class, we initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model=LogisticRegressionModel(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate “lr” is set to 0.01, but it's a hyperparameter that should be fine-tuned to an optimal value. The number of epochs is set to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCEWithLogitsLoss()\n",
    "optimizer=torch.optim.SGD(lr_model.parameters(),lr=0.01)\n",
    "\n",
    "number_of_epochs=1000\n",
    "for epoch in range(number_of_epochs):\n",
    " optimizer.zero_grad()\n",
    " y_prediction=lr_model(x_train)\n",
    " loss=criterion(y_prediction,y_train)\n",
    " loss.backward()\n",
    " optimizer.step()\n",
    " if (epoch+1)%50 == 0 or epoch==0:\n",
    "    # Calculate Accuracy         \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Iterate through train dataset\n",
    "    for label in y_train:\n",
    "        # Get predictions and compare with the churn label\n",
    "        predicted = y_prediction[total]\n",
    "        #print(label, predicted, (predicted >=0.5) and (label == 1))\n",
    "        total += 1\n",
    "        if (predicted >=0.5) and (label == 1) or (predicted <0.5) and (label == 0) :\n",
    "            correct += 1\n",
    "        \n",
    "        \n",
    "    accuracy = 100 * correct / total\n",
    "    print('epoch:', epoch+1,',loss=',loss.item(),',accuracy=',accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the S-curve for cutomer activity level vs. churn. Matplotlib and Seaborn are two common plotting libraries used in Python. These plotting libraries are useful in creating custom visualizations to help gain insights from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "y_prediction = lr_model(x_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "X = test_df['ACTIVITY_LEVEL']\n",
    "Y = y_prediction.detach().numpy() #test_df['CHURN']\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Activity Level\", fontsize=16)\n",
    "plt.ylabel(\"Probability of Churn\", fontsize=16)\n",
    "sns.regplot(x=np.array(X),y=np.array(Y), order=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
